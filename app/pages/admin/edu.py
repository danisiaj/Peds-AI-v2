## Import necessary libraries ##
import streamlit as st
import pandas as pd
import sqlite3
import mysql.connector
from openai import OpenAI

## OpenAI API KEY ##
API_KEY = st.secrets.openai_api_key

## Define the functions ##
def set_up_page():
    """ This function builds the header for the page"""

    st.header("Education Center")
 
def load_data():
    """
    This function loads a dataset with all the nurses information 
    It also builds a dataframe widget in the app for visualization purposes
    """

    nurses_data = pd.read_csv('./data/nurses_dataset.csv')
    st.markdown('### _My Nurses:_')
    st.dataframe(nurses_data.head(), use_container_width=True, hide_index=True)

def define_db_config():
    """
    Set up MyQSL connection configuration

    Returns:
        - db_config: dict
    """

    db_config = {
        "host": "localhost",
        "user": "root",
        "password": st.secrets.sql_password,
        "database": "nurses"
    }
    
    return db_config

def setup_database_for_user_query(db_config):
    """
    This function builds the table in the dataset in MySQL to store any new question from the user
    *** QUESTIONS COME FROM THE PEDS_AI PAGE

    Arguments:
        - db_config: dict, information to connect to MySQL
    """


    # Establish connection with MySQL
    connection = mysql.connector.connect(**db_config)
    cursor = connection.cursor()

    # Build the query
    cursor.execute("""
    DROP TABLE IF EXISTS user_questions;

    CREATE TABLE IF NOT EXISTS user_questions (
        id INT AUTO_INCREMENT PRIMARY KEY,
        user VARCHAR(255),
        question TEXT,
        topic TEXT
    )
    """)

    # Execute the query
    connection.close()

def store_user_question(db_config, df):
    """
    This function takes the dataframe with all the questions from the users and update the database in MyQSL
    *** QUESTIONS COME FROM THE PEDS_AI PAGE

    Arguments:
        - db_config: dict, information to connect to MySQL
    """

    # Establish connection
    connection = mysql.connector.connect(**db_config)

    # Upload DataFrames to SQL using pymysql connection
    try:
        with connection.cursor() as cursor:
            # Insert data into personal_info table
            queries_columns = ', '.join(df.columns)

            for index, row in df.iterrows():
                query_store_queries = f"INSERT INTO personal_info ({queries_columns}) VALUES ({', '.join(['%s'] * len(df.columns))})"
                cursor.execute(query_store_queries, tuple(row))
            
            # Commit the transaction
            connection.commit()
            print("DataFrames uploaded successfully.")
    except Exception as e:
        print("Error while uploading DataFrames:", e)
    finally:
        connection.close() 

def execute_sql_query(sql_query):
    """
    This function executes the sql_query generated by a filter chosen by the user in the app
    
    Arguments:
        - sql_query: str

    Returns: 
        - df: Dataframe with the filter information
    """

    # Connect to the SQLite database
    conn = sqlite3.connect(':memory:')
    cursor = conn.cursor()
    
    # Load and execute SQL script
    with open('./data/nurses_1.sql') as file:
        sql_script = file.read()
    cursor.executescript(sql_script)
    conn.commit()
    
    # Execute the specific query and fetch results into a DataFrame
    df = pd.read_sql_query(sql_query, conn)
    
    # Close the connection after retrieving data
    conn.close()

    return df

def sql_query(selection):
    """
    This function generates a dynamic SQL query based on the selected certification from user in app

    Arguments:
        - selection: str

    Returns:
        - sql_query: str
    """

    sql_query = f"""
        SELECT  
            personal_info._id as ID,
            last_name as 'Last Name',
            first_name as 'First Name'
        FROM personal_info
        RIGHT JOIN (
            SELECT
                education_info._id,
                {selection.lower()} AS {selection.upper()}
            FROM education_info 
            WHERE {selection.lower()} = 0) as education
        ON personal_info._id = education._id 
        ORDER BY last_name ASC;
    """

    return sql_query

def certifications_filters():
    """
    - Build the options for the user to filter information about the nurses certifications
    - Generate the sql_query
    - Execute the query
    - build the Dataframe with the filter information
    """

    certifications = ["BLS", "PALS", "ACEs", "Cardiology Hours", "Transplant Hours", "Nephrology Hours", "Respiratory Hours"]
    selection = st.selectbox("Filter by Certifications", certifications)  # Select certification

    if selection:
        st.markdown(f'### _Nurses missing {selection.upper()}:_')
        query = sql_query(selection.replace(" ", "_"))  # Generate SQL query
        df = execute_sql_query(query)  # Execute and get the result DataFrame
        st.write("Total nurses:", len(df))  # Display count
        st.dataframe(df, use_container_width=True, hide_index=True)  # Display DataFrame

def analyze_queries():
    """
    This functions builds a dynamic prompt based on the user's questions to create a chat with OpenAI

    Returns:
        analysis: str
    """

    queries = '\n'
    for user, question in zip(st.session_state.nurses_data.user, st.session_state.nurses_data.question):
        queries += '\Queries:\n'
        queries += f'user: {str(user)} -> {str(question)}\n\n'

    prompt = f"""
    You are a data analyst specializing in user behavior analysis. 
    I will provide you with a series of queries made by users. Your task is to:

    1. Analyze the queries to extract the **key words** from each query, ignoring all stopwords.
    2. Identify the **most common words** across all the queries after removing stopwords.
    3. Generate a **brief paragraph summarizing the most common topics** that users inquire about based on the extracted key words.
    4. Propose **two actionable recommendations** for improving training and education related to the identified topics.

    Here are the user queries:
        {queries}     
    Provide your response in the following format:

    - **Key Words**: [List of key words extracted from the queries]
    - **Most Common Words**: [List of the most common words after filtering stopwords]
    - **Summary**: [A concise paragraph summarizing the common topics in user queries]
    - **Recommendations**:
    1. [First recommendation for training and education]
    2. [Second recommendation for training and education]

    Remember to return the answer in markdown format.
    """

    client_openai = OpenAI(api_key = API_KEY)

    messages = [{'role':'user', 'content':prompt}]
    model_params = {'model': 'gpt-4o-mini', 'temperature': 0.4, 'max_tokens': 500}
    completion = client_openai.chat.completions.create(messages=messages, **model_params, timeout=120)

    answer = completion.choices[0].message.content

    return answer

def query_history():
    """
    This function stores the user, question and the topic in a dataframe. 
    The dataframe is printed for visualizatino purposes and stored in MySQL database
    """

    st.markdown('### _Query History:_')
    # Initialize 'nurses_data' in session state if it doesn't exist
    if 'nurses_data' not in st.session_state:
        st.session_state.nurses_data = pd.DataFrame(columns=['user', 'question'])
        if len(st.session_state.nurses_data) == 0:
            st.markdown("** Query history empty **")
        
    # Check if user and query are not None before continuing
    if st.session_state.user is not None and st.session_state.query is not None:        
        # Check if the combination of 'user' and 'question' in new_row already exists in nurses_data
        if not ((st.session_state.nurses_data.user == st.session_state.user) & 
                (st.session_state.nurses_data.question == st.session_state.query)).any():
            # Merge only if it doesn't exist
            new_row = pd.DataFrame([{
                                'user': st.session_state.user, 
                                'question': st.session_state.query,
                                'topic':st.session_state.collection
                                }])

            st.session_state.nurses_data = pd.concat([st.session_state.nurses_data, new_row], ignore_index=True)
            
            # Update MySQL database with the new datapoint
            setup_database_for_user_query()
            store_user_question(new_row) 

        else:
            pass      


        # Display the updated dataframe
        st.dataframe(st.session_state.nurses_data, use_container_width=True)

        # Option for the user to analyze the questions through an LLM
        if st.button('Analyze queries') and len(st.session_state.nurses_data) > 0:
            analysis = analyze_queries()
            st.markdown(analysis)
   
    elif st.session_state.query is None:
        pass

def main():

    set_up_page()
    load_data()
    certifications_filters()
    query_history()

## Initialize the app   
main()